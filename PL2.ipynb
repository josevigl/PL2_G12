{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos del equipo (COMPLETAR)\n",
    "\n",
    "Identificador de grupo: Grupo 12.\n",
    "\n",
    "| Nombre y apellido | % de contribución |\n",
    "|-------------------|-------------------|\n",
    "|       Marcelo Chinarro Cabrero            |      20             |\n",
    "|       José Vicente García López            |        20           |\n",
    "|         Antonio Gómez Jimeno          |           20        |\n",
    "|      Álvaro Mayorga Zarco             |         20          |\n",
    "|    Marcos Redondo Madruga               |       20            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. INSTALACIÓN Y CONFIGURACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision hypothesis adversarial-robustness-toolbox adversarial-robustness-toolbox[pytorch] scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CONFIGURACIÓN INICIAL Y CARGA DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "from hypothesis import given, strategies as st, settings\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sklearn.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATOS Y MODELO BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.63MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 311kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.08MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.11MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 30000, Test: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "\n",
    "# Cargar MNIST (versión simplificada, mitad del dataset)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_full = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_full = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_indices = np.random.choice(len(train_full), 30000, replace=False)\n",
    "test_indices = np.random.choice(len(test_full), 5000, replace=False)\n",
    "\n",
    "train_subset = Subset(train_full, train_indices)\n",
    "test_subset = Subset(test_full, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_subset)}, Test: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros: 109,386\n"
     ]
    }
   ],
   "source": [
    "# Modelo simple\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Red neuronal simple para MNIST\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        # Capa 1\n",
    "        self.flatten = nn.Flatten()         # [1,28,28] → [784]\n",
    "        self.fc1 = nn.Linear(28*28, 128)    # [784] → [128]\n",
    "        self.relu1 = nn.ReLU()              # Activación no-lineal\n",
    "        self.dropout1 = nn.Dropout(0.2)     # Regularización\n",
    "\n",
    "        # Capa 2\n",
    "        self.fc2 = nn.Linear(128, 64)   # [128] → [64]\n",
    "        self.relu2 = nn.ReLU()          # Activación no-lineal\n",
    "        self.dropout2 = nn.Dropout(0.2) # Regularización\n",
    "\n",
    "        # Capa salida\n",
    "        self.fc3 = nn.Linear(64, 10) # [64] → [10] (10 clases)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "print(f\"Parámetros: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo baseline...\n",
      "  Batch 0/235, Loss: 2.3157\n",
      "  Batch 100/235, Loss: 0.4275\n",
      "  Batch 200/235, Loss: 0.3476\n",
      "Época 1: Loss: 0.687, Accuracy: 80.2%\n",
      "  Batch 0/235, Loss: 0.3925\n",
      "  Batch 100/235, Loss: 0.2804\n",
      "  Batch 200/235, Loss: 0.3322\n",
      "Época 2: Loss: 0.304, Accuracy: 91.1%\n",
      "  Batch 0/235, Loss: 0.2534\n",
      "  Batch 100/235, Loss: 0.1831\n",
      "  Batch 200/235, Loss: 0.3052\n",
      "Época 3: Loss: 0.236, Accuracy: 93.0%\n",
      "  Batch 0/235, Loss: 0.1322\n",
      "  Batch 100/235, Loss: 0.1779\n",
      "  Batch 200/235, Loss: 0.2654\n",
      "Época 4: Loss: 0.189, Accuracy: 94.3%\n",
      "  Batch 0/235, Loss: 0.1647\n",
      "  Batch 100/235, Loss: 0.2160\n",
      "  Batch 200/235, Loss: 0.1596\n",
      "Época 5: Loss: 0.158, Accuracy: 95.2%\n",
      "\n",
      "Accuracy: 96.30%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=3):\n",
    "    \"\"\"Entrena el modelo\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'  Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "\n",
    "        acc = 100. * correct / total\n",
    "        print(f'Época {epoch+1}: Loss: {running_loss/len(train_loader):.3f}, '\n",
    "              f'Accuracy: {acc:.1f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_simple(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "print(\"Entrenando modelo baseline...\")\n",
    "train_model(model, train_loader, epochs=5)\n",
    "acc = evaluate_simple(model, test_loader)\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PROPERTY-BASED TESTING (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test de consistencia: PASADO\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "\n",
    "def predict_single(model, image):\n",
    "    \"\"\"Predice una imagen y retorna clase y confianza\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred_class].item()\n",
    "    return pred_class, confidence\n",
    "\n",
    "@given(st.integers(min_value=0, max_value=len(test_subset)-1))\n",
    "@settings(max_examples=20, deadline=None)\n",
    "def test_consistency(idx):\n",
    "    # EJERCICIO 1: Verifica que dos predicciones de la misma imagen sean idénticas.\n",
    "    # 1. Obtener imagen\n",
    "    # 2. Predecir dos veces\n",
    "    # 3. Verificar con assert\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    image, _ = test_subset[idx]\n",
    "    image = image.to(device)\n",
    "    \n",
    "    pred1, conf1 = predict_single(model, image)\n",
    "    pred2, conf2 = predict_single(model, image)\n",
    "    \n",
    "    assert pred1 == pred2, f\"Predicciones inconsistentes: {pred1} vs {pred2}\"\n",
    "    assert abs(conf1 - conf2) < 1e-6, f\"Confianzas inconsistentes: {conf1} vs {conf2}\"\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "    \n",
    "\n",
    "# Ejecutar test\n",
    "try:\n",
    "    test_consistency()\n",
    "    print(\"✅ Test de consistencia: PASADO\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test fallado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PROPERTY-BASED TESTING (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Test fallado: Confianza cayó demasiado: 0.996 → 0.340 (ruido=0.266)\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "\n",
    "@given(\n",
    "    st.integers(min_value=0, max_value=len(test_subset)-1),\n",
    "    st.floats(min_value=0.01, max_value=0.3)\n",
    ")\n",
    "@settings(max_examples=15, deadline=None)\n",
    "def test_noise_robustness(idx, noise_std):\n",
    "    # EJERCICIO 2: Verifica que un pequeño ruido no destruya la confianza del modelo.\n",
    "    # 1. Obtener imagen\n",
    "    # 2. Crear imagen con ruido, por ejemplo: image_noisy = image + torch.randn_like(image) * noise_std\n",
    "    # 3. Predecir ambas\n",
    "    # 4. Verificar que la confianza no baja más de un 50% con respecto de la original\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    image, _ = test_subset[idx]\n",
    "    image = image.to(device)\n",
    "    \n",
    "    _, conf_original = predict_single(model, image)\n",
    "    \n",
    "    noise = torch.randn_like(image) * noise_std\n",
    "    image_noisy = torch.clamp(image + noise, 0, 1)\n",
    "    \n",
    "    _, conf_noisy = predict_single(model, image_noisy)\n",
    "    \n",
    "    assert conf_noisy >= 0.5 * conf_original, \\\n",
    "        f\"Confianza cayó demasiado: {conf_original:.3f} → {conf_noisy:.3f} (ruido={noise_std:.3f})\"\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "    \n",
    "\n",
    "try:\n",
    "    test_noise_robustness()\n",
    "    print(\"✅ Test de ruido: PASADO\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Test fallado: {str(e)[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. VALIDACIÓN ESTADÍSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando bootstrap...\n",
      "\n",
      "Accuracy: 96.31% [IC 95%: 95.78% - 96.78%]\n",
      "Ancho del intervalo: 1.00%\n"
     ]
    }
   ],
   "source": [
    "# Obtener predicciones para bootstrap\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "def bootstrap_accuracy(preds, labels, n_iterations=1000):\n",
    "    # EJERCICIO 3: Calcula intervalo de confianza 95% para accuracy.\n",
    "    # 1. Crear lista vacía: accuracies = []\n",
    "    # 2. Iterar n_iterations veces\n",
    "    # 3. Resamplear: indices = resample(range(len(preds)), n_samples=len(preds))\n",
    "    # 4. Calcular accuracy: acc = accuracy_score(labels[indices], preds[indices])\n",
    "    # 5. Añadir accuracy a la lista\n",
    "    # 6. Retornar tupla (media, percentil 2.5, percentil 97.5)\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    accuracies = []\n",
    "    n = len(preds)\n",
    "    for _ in range(n_iterations):\n",
    "        indices = resample(range(n), n_samples=n, replace=True)\n",
    "        acc = accuracy_score(labels[indices], preds[indices])\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    mean = np.mean(accuracies)\n",
    "    lower = np.percentile(accuracies, 2.5)\n",
    "    upper = np.percentile(accuracies, 97.5)\n",
    "    return mean, lower, upper\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "    \n",
    "\n",
    "# Ejecutar bootstrap\n",
    "print(\"Calculando bootstrap...\")\n",
    "mean, lower, upper = bootstrap_accuracy(all_preds, all_labels, n_iterations=1000)\n",
    "print(f\"\\nAccuracy: {mean*100:.2f}% [IC 95%: {lower*100:.2f}% - {upper*100:.2f}%]\")\n",
    "print(f\"Ancho del intervalo: {(upper-lower)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPARACIÓN CON BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparando modelos...\n",
      "\n",
      "Modelo 1: 96.31%\n",
      "Modelo 2: 95.30%\n",
      "¿Son significativamente diferentes? NO\n"
     ]
    }
   ],
   "source": [
    "# Simulamos un segundo modelo (copia con ruido en predicciones)\n",
    "# En la práctica real, serían predicciones de otro modelo\n",
    "model2_preds = all_preds.copy()\n",
    "# Simular que el modelo 2 comete algunos errores diferentes\n",
    "flip_indices = np.random.choice(len(model2_preds), size=50, replace=False)\n",
    "model2_preds[flip_indices] = (model2_preds[flip_indices] + 1) % 10\n",
    "\n",
    "def compare_models(preds1, preds2, labels):\n",
    "    # EJERCICIO 4: Compara estadísticamente dos modelos.\n",
    "    # 1. Calcular bootstrap para modelo 1 con 500 iteraciones\n",
    "    # 2. Calcular bootstrap para modelo 2 con 500 iteraciones\n",
    "    # 3. Comprobar si intervalos NO se solapan (no_overlap)\n",
    "    # 4. Retornar no_overlap y ambas medias\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    acc1_list = []\n",
    "    n = len(preds1)\n",
    "    for _ in range(500):\n",
    "        idx = resample(range(n), n_samples=n, replace=True)\n",
    "        acc = accuracy_score(labels[idx], preds1[idx])\n",
    "        acc1_list.append(acc)\n",
    "    mean1 = np.mean(acc1_list)\n",
    "    lower1 = np.percentile(acc1_list, 2.5)\n",
    "    upper1 = np.percentile(acc1_list, 97.5)\n",
    "    \n",
    "    acc2_list = []\n",
    "    for _ in range(500):\n",
    "        idx = resample(range(n), n_samples=n, replace=True)\n",
    "        acc = accuracy_score(labels[idx], preds2[idx])\n",
    "        acc2_list.append(acc)\n",
    "    mean2 = np.mean(acc2_list)\n",
    "    lower2 = np.percentile(acc2_list, 2.5)\n",
    "    upper2 = np.percentile(acc2_list, 97.5)\n",
    "    \n",
    "    no_overlap = upper1 < lower2 or upper2 < lower1\n",
    "    \n",
    "    return no_overlap, mean1, mean2\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "\n",
    "print(\"Comparando modelos...\")\n",
    "different, acc1, acc2 = compare_models(all_preds, model2_preds, all_labels)\n",
    "print(f\"\\nModelo 1: {acc1*100:.2f}%\")\n",
    "print(f\"Modelo 2: {acc2*100:.2f}%\")\n",
    "print(f\"¿Son significativamente diferentes? {'SÍ' if different else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EVALUACIÓN ADVERSARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "# Preparar datos sin normalización para ART\n",
    "transform_art = transforms.ToTensor()\n",
    "test_art = datasets.MNIST('./data', train=False, transform=transform_art)\n",
    "test_subset_art = Subset(test_art, test_indices[:500])  # Solo 500 para rapidez\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for img, label in test_subset_art:\n",
    "    X_test.append(img.numpy())\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Datos para ART: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper para ART (normaliza internamente)\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalizar\n",
    "        x = (x - 0.1307) / 0.3081\n",
    "        return self.model(x)\n",
    "\n",
    "wrapped = ModelWrapper(model)\n",
    "\n",
    "# Crear clasificador ART\n",
    "classifier = PyTorchClassifier(\n",
    "    model=wrapped,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(model.parameters()),\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10,\n",
    "    clip_values=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "print(\"✅ Clasificador ART listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ATAQUE FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fgsm(classifier, X_test, y, epsilon=0.3):\n",
    "    # EJERCICIO 5: Evalúa robustez con ataque FGSM.\n",
    "    # 1. Crear ataque indicando solo estimator y eps\n",
    "    # 2. Generar adversarios X_adv (usar X_test como input)\n",
    "    # 3. Predecir ambos (limpios y adversarios) y calcular accuracies\n",
    "    # 4. Retornar acc_clean, acc_adv\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "    pass\n",
    "\n",
    "print(\"Ejecutando ataque FGSM...\")\n",
    "clean_acc, adv_acc = evaluate_fgsm(classifier, X_test, y_test, epsilon=0.3)\n",
    "print(f\"\\nAccuracy limpia: {clean_acc:.2f}%\")\n",
    "print(f\"Accuracy adversaria (FGSM): {adv_acc:.2f}%\")\n",
    "print(f\"Caída: {clean_acc - adv_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ATAQUE PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pgd(classifier, X, y, epsilon=0.3):\n",
    "    # EJERCICIO 6: Evalúa robustez con ataque PGD.\n",
    "    # 1. Crear ataque indicando solo estimator, eps, eps step (epsilon/10) y 40 iteraciones\n",
    "    # 2. Generar y evaluar como en FGSM\n",
    "    # 3. Retornar acc_clean, acc_adv\n",
    "    # === INICIO SOLUCIÓN ===\n",
    "\n",
    "    # === FIN SOLUCIÓN ===\n",
    "    pass\n",
    "\n",
    "print(\"Ejecutando ataque PGD...\")\n",
    "clean_acc_pgd, adv_acc_pgd = evaluate_pgd(classifier, X_test, y_test, epsilon=0.3)\n",
    "print(f\"\\nAccuracy limpia: {clean_acc_pgd:.2f}%\")\n",
    "print(f\"Accuracy adversaria (PGD): {adv_acc_pgd:.2f}%\")\n",
    "print(f\"Caída: {clean_acc_pgd - adv_acc_pgd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ADVERSARIAL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "\n",
    "# Preparar datos para adversarial training\n",
    "print(\"\\nPreparando datos para adversarial training...\")\n",
    "\n",
    "# Preparar datos de train para ART\n",
    "train_art = datasets.MNIST('./data', train=True, transform=transforms.ToTensor())\n",
    "train_subset_art = Subset(train_art, train_indices)\n",
    "train_loader_art = DataLoader(train_subset_art, batch_size=128, shuffle=True)\n",
    "\n",
    "# Extraer datos de entrenamiento en formato numpy\n",
    "X_train = []\n",
    "y_train = []\n",
    "for img, label in train_subset_art:\n",
    "    X_train.append(img.numpy())\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "\n",
    "# Crear nuevo modelo robusto\n",
    "robust_model = SimpleNN().to(device)\n",
    "wrapped_robust = ModelWrapper(robust_model)\n",
    "\n",
    "# Crear clasificador para adversarial training\n",
    "classifier_robust = PyTorchClassifier(\n",
    "    model=wrapped_robust,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(robust_model.parameters(), lr=0.001),\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10,\n",
    "    clip_values=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "attack_train = FastGradientMethod(estimator=classifier_robust, eps=0.1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVERSARIAL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Entrenando modelo robusto...\\n\")\n",
    "\n",
    "# EJERCICIO 7: Entrenar modelo con ejemplos adversarios.\n",
    "# 1. Crear AdversarialTrainer indicando classifier, attack_train y ratio=0.5\n",
    "# 2. Entrenar con fit usando X_train, y_train, nb_epochs=3, batch_size=128\n",
    "# === INICIO SOLUCIÓN ===\n",
    "\n",
    "# === FIN SOLUCIÓN ===\n",
    "\n",
    "print(\"\\n✅ Adversarial training completado\")\n",
    "\n",
    "# Evaluar modelo robusto en datos limpios\n",
    "print(\"\\nEvaluando modelo robusto...\")\n",
    "acc_robust = evaluate_simple(robust_model, test_loader)\n",
    "print(f\"Accuracy en test limpio: {acc_robust:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. COMPARACIÓN FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo robusto\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACIÓN: BASELINE VS ROBUSTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear clasificador para modelo robusto\n",
    "wrapped_robust_eval = ModelWrapper(robust_model)\n",
    "classifier_robust_eval = PyTorchClassifier(\n",
    "    model=wrapped_robust_eval,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(robust_model.parameters()),\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10,\n",
    "    clip_values=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "# Evaluar ambos modelos\n",
    "print(\"\\nModelo BASELINE:\")\n",
    "base_clean, base_fgsm = evaluate_fgsm(classifier, X_test, y_test, 0.3)\n",
    "_, base_pgd = evaluate_pgd(classifier, X_test, y_test, 0.3)\n",
    "print(f\"  Limpia: {base_clean:.2f}%\")\n",
    "print(f\"  FGSM:   {base_fgsm:.2f}%\")\n",
    "print(f\"  PGD:    {base_pgd:.2f}%\")\n",
    "\n",
    "print(\"\\nModelo ROBUSTO:\")\n",
    "rob_clean, rob_fgsm = evaluate_fgsm(classifier_robust_eval, X_test, y_test, 0.3)\n",
    "_, rob_pgd = evaluate_pgd(classifier_robust_eval, X_test, y_test, 0.3)\n",
    "print(f\"  Limpia: {rob_clean:.2f}%\")\n",
    "print(f\"  FGSM:   {rob_fgsm:.2f}%\")\n",
    "print(f\"  PGD:    {rob_pgd:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEJORAS:\")\n",
    "print(f\"  Limpia: {rob_clean - base_clean:+.2f}%\")\n",
    "print(f\"  FGSM:   {rob_fgsm - base_fgsm:+.2f}%\")\n",
    "print(f\"  PGD:    {rob_pgd - base_pgd:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "# Visualización final\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Limpia', 'FGSM', 'PGD']\n",
    "baseline_scores = [base_clean, base_fgsm, base_pgd]\n",
    "robust_scores = [rob_clean, rob_fgsm, rob_pgd]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline', color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, robust_scores, width, label='Robusto', color='lightgreen')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Comparación: Modelo Baseline vs Robusto', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores sobre barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. EJERCICIO 8\n",
    "\n",
    "Responde a las siguientes preguntas en cada una de las propias celdas Markdown\n",
    "del Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Property-Based Testing\n",
    "\n",
    "- ¿Pasaron ambos tests (ruido y consistencia)? Si alguno falló, ¿por qué crees que ocurrió?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿Qué ventaja tiene usar Hypothesis vs. tests manuales?\n",
    "    > [Respuesta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Validación Estadística\n",
    "\n",
    "- ¿Cuál es el intervalo de confianza de la accuracy? ¿Es estrecho o amplio?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿Por qué es importante reportar intervalos y no solo la media?\n",
    "    > [Respuesta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Robustez Adversaria\n",
    "\n",
    "- ¿Cuánto cae la accuracy bajo ataques FGSM?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿PGD empeoró con FGSM training? ¿Por qué?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿Cuánto mejora el adversarial training la robustez?\n",
    "    > [Respuesta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compromisos\n",
    "\n",
    "- ¿El modelo robusto perdió accuracy limpia? ¿Cuánto?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿Vale la pena el Compromisos para aplicaciones reales?\n",
    "    > [Respuesta]\n",
    "\n",
    "- ¿En qué casos recomendarías usar adversarial training?\n",
    "    > [Respuesta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONES\n",
    "\n",
    "En esta práctica hemos:\n",
    "1. Verificado invariantes del modelo con property-based testing\n",
    "2. Validado rendimiento estadísticamente con intervalos de confianza\n",
    "3. Expuesto la vulnerabilidad extrema a ataques adversarios\n",
    "4. Logrado robustez mediante adversarial training\n",
    "\n",
    "**Reflexión final**: La validación rigurosa —incluyendo robustez adversaria— es crítica antes de desplegar modelos en producción. Un modelo preciso pero vulnerable es un sistema inseguro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
